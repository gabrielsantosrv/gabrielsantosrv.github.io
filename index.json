[{"authors":null,"categories":null,"content":"I am a PhD student in Computer Science at Artificial Intelligence (Recod.ai) Lab in the Institute of Computing at the University of Campinas (Unicamp) under the supervision of Professor Sandra Avila and co-supervision of Professor Esther Colombini. Also, I am a member of the Artificial Intelligence and Cognitive Architectures Hub (H.IAAC). I am interested in Natural Language Processing (NLP) and Multimodal Representation Learning. Lately, my research has been focusing on adapting multimodal models to support low-resource languages, with a particular interest in enhancing resources for Portuguese.\n","date":1702339200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1702339200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a PhD student in Computer Science at Artificial Intelligence (Recod.ai) Lab in the Institute of Computing at the University of Campinas (Unicamp) under the supervision of Professor Sandra Avila and co-supervision of Professor Esther Colombini.","tags":null,"title":"Gabriel Oliveira","type":"authors"},{"authors":["Gabriel Oliveira","Diego A. B. Moreira","Alef Iury Ferreira","Jhessica Silva","Luiz Pereira","Pedro Bueno","Thiago Sousa","Helena Maia","Nádia Da Silva","Esther Colombini","Helio Pedrini","Sandra Avila"],"categories":null,"content":"","date":1702339200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702339200,"objectID":"8370b62c1d3d63d78fa1de26fc419074","permalink":"https://gabrielsantosrv.github.io/publication/capivara/","publishdate":"2023-12-12T00:00:00Z","relpermalink":"/publication/capivara/","section":"publication","summary":"This work introduces CAPIVARA, a cost-efficient framework designed to enhance the performance of\nmultilingual CLIP models in low-resource languages. While CLIP has excelled in zero-shot \nvision-language tasks, the resource-intensive nature of model training remains challenging. \nMany datasets lack linguistic diversity, featuring solely English descriptions for images. \nCAPIVARA addresses this by augmenting text data using image captioning and machine translation to \ngenerate multiple synthetic captions in low-resource languages. We optimize the training pipeline \nwith LiT, LoRA, and gradient checkpointing to alleviate the computational cost. Through extensive \nexperiments, CAPIVARA emerges as state of the art in zero-shot tasks involving images and \nPortuguese texts. We show the potential for significant improvements in other low-resource \nlanguages, achieved by fine-tuning the pre-trained multilingual CLIP using CAPIVARA on a single GPU \nfor 2 hours. Our model and code is available at https: //github.com/hiaac-nlp/CAPIVARA.\n","tags":["CAPIVARA","CLIP","Multimodal Representation Learning","Zero-shot cross-modal retrieval","Zero-shot image classification"],"title":"CAPIVARA: Cost-Efficient Approach for Improving Multilingual CLIP Performance on Low-Resource Languages","type":"publication"},{"authors":["Gabriel Oliveira","Esther Colombini","Sandra Avila"],"categories":null,"content":"","date":1642723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642723200,"objectID":"eadffc7000f3d589c5ec1e87f7a12f11","permalink":"https://gabrielsantosrv.github.io/publication/pracegover/","publishdate":"2022-01-21T00:00:00Z","relpermalink":"/publication/pracegover/","section":"publication","summary":"Automatically describing images using natural sentences is essential to visually impaired\npeople’s inclusion on the Internet. This problem is known as Image Captioning. There are many\ndatasets in the literature, but most contain only English captions, whereas datasets with captions\ndescribed in other languages are scarce. We introduce the #PraCegoVer, a multi-modal dataset with\nPortuguese captions based on posts from Instagram. It is the first large dataset for image\ncaptioning in Portuguese. In contrast to popular datasets, #PraCegoVer has only one reference per\nimage, and both mean and variance of reference sentence length are significantly high, which makes\nour dataset challenging due to its linguistic aspect. We carry a detailed analysis to find the main\nclasses and topics in our data. We compare #PraCegoVer to MS COCO dataset in terms of sentence\nlength and word frequency. We hope that #PraCegoVer dataset encourages more works addressing the\nautomatic generation of descriptions in Portuguese.\n","tags":["Image Captioning","Dataset"],"title":"#PraCegoVer: A Large Dataset for Image Captioning in Portuguese","type":"publication"},{"authors":["Gabriel Oliveira"],"categories":["Deep Learning","Neural Networks"],"content":"","date":1641340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641340800,"objectID":"8b5c4ff54038c5275b976832ec78de71","permalink":"https://gabrielsantosrv.github.io/post/fundamental_nn/","publishdate":"2022-01-05T00:00:00Z","relpermalink":"/post/fundamental_nn/","section":"post","summary":"Inspired by the human brain, Artificial Neural Networks are powerful models that excel at\nrecognizing patterns. Recently, thanks to advances in parallel computing, these models have been\nlargely employed to solve many complex problems in different fields such as Computer Vision, Natural\nLanguage Processing, Robotics, Drug Discovery, etc. In this article, we are going to describe the\ntheoretical fundamentals of Artificial Neural Networks.\n","tags":null,"title":"Fundamentals of Neural Networks","type":"post"},{"authors":["Gabriel Oliveira"],"categories":["Deep Learning","Transformers"],"content":"","date":1636588800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636588800,"objectID":"bac0391f6cb13b704606a31d0070a803","permalink":"https://gabrielsantosrv.github.io/post/transformers/","publishdate":"2021-11-11T00:00:00Z","relpermalink":"/post/transformers/","section":"post","summary":"The Transformer architecture is considered a breakthrough and has been largely employed in NLP and,\nmore recently, in Computer Vision areas. So, in this article, we are going to focus on Transformers\narchitecture and hack it.\n","tags":null,"title":"Attention to Transformer, please!","type":"post"},{"authors":["Gabriel Oliveira","Esther Colombini","Sandra Avila"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"f0c18c0c6c6fbeba4f8694bf404d610b","permalink":"https://gabrielsantosrv.github.io/publication/cider_r/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/publication/cider_r/","section":"publication","summary":"This paper shows that CIDEr-D, a traditional evaluation metric for image description, does not work\nproperly on datasets where the number of words in the sentence is significantly greater than those\nin the MS COCO Captions dataset. We also show that CIDEr-D has performance hampered by the lack of\nmultiple reference sentences and high variance of sentence length. To bypass this problem, we\nintroduce CIDEr-R, which improves CIDEr-D, making it more flexible in dealing with datasets with\nhigh sentence length variance. We demonstrate that CIDEr-R is more accurate and closer to human\njudgment than CIDEr-D; CIDEr-R is more robust regarding the number of available references. Our\nresults reveal that using Self-Critical Sequence Training to optimize CIDEr-R generates descriptive\ncaptions. In contrast, when CIDEr-D is optimized, the generated captions’ length tends to be similar\nto the reference length. However, the models also repeat several times the same word to increase the\nsentence length.\n","tags":["Image Captioning","Metrics"],"title":"CIDEr-R: Robust Consensus-based Image Description Evaluation","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8e7bc052bdfc6746ea2bb6595e8093eb","permalink":"https://gabrielsantosrv.github.io/home/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]